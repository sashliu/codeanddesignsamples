1. One issue that I can think of is the fact that the reducers will be unequally distributed. With reducer1 receiving all pairs where keys are between A and M, and reducer2 receiving all pairs with keys between N and Z, and with the keys being months, a huge amount of data will be pushed into reducer1 as 9 months fall between the letters A and M. Essentially when processing 4 million records, one reducer will be receiving much more data than the other. To fix this, we could change the range of the letters to be something like A through J and K through Z for reducer1 and reducer2 respectively. This would make it so 7 of the months would go to reducer1 and 5 would go to reducer2, creating a much more equal distribution.

2. The output is not correct because it is not producing the most frequent words for each letter, but just 10 words not accounting for their frequency. If we want to get the 10 most frequent words, we have to sort the list in a descending order (so the words with the most count are at the top) before we grab the top 10.
